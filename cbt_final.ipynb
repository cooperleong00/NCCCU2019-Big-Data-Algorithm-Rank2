{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import catboost as cbt\n",
    "import lightgbm as lgb\n",
    "import time\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score,log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder,MinMaxScaler\n",
    "\n",
    "from itertools import combinations,permutations\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def reduce_mem_usage(data):\n",
    "    '''\n",
    "    通过判断数据范围的上下限来选择最小能存储数据的类型\n",
    "    注意:在存储feather前不要使用,因为feather不支持float16位类型\n",
    "    data:输入dataframe\n",
    "    return:返回优化后的dataframe\n",
    "    '''\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = data.memory_usage().sum() / 1024**2    \n",
    "    for col in tqdm(data.columns):\n",
    "        col_type = data[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = data[col].min()\n",
    "            c_max = data[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    data[col] = data[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    data[col] = data[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    data[col] = data[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    data[col] = data[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    #为避免feather不支持此类型\n",
    "                    #data[col] = data[col].astype(np.float16)\n",
    "                    pass\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    data[col] = data[col].astype(np.float32)\n",
    "                else:\n",
    "                    data[col] = data[col].astype(np.float64)    \n",
    "    end_mem = data.memory_usage().sum() / 1024**2\n",
    "    print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "label = pd.read_csv('train_label.csv')\n",
    "train['label'] = label.label\n",
    "data = pd.concat([train,test],axis=0,sort=False).reset_index(drop=True)\n",
    "\n",
    "data['date'] =pd.to_datetime(data['date'])\n",
    "data['hour'] = data['date'].dt.hour\n",
    "data['day'] = data['date'].dt.day\n",
    "\n",
    "A_ft = ['A'+str(i) for i in range(1,4)]\n",
    "B_ft = ['B'+str(i) for i in range(1,4)]\n",
    "C_ft = ['C'+str(i) for i in range(1,4)]\n",
    "D_ft = ['D1','D2']\n",
    "\n",
    "cat_list = A_ft+B_ft+C_ft+['hour','day']+['E'+str(i) for i in [1,6,14,20,27]]+['E'+str(i) for i in [4,11,12,24,26,28]]+['E'+str(i) for i in [8,15,18,25]]+['E'+str(i) for i in [23,29]]\n",
    "\n",
    "numerical_cols = [col for col in data.columns if col not in ['ID','label','date']+cat_list+['D1','D2']]\n",
    "\n",
    "data[numerical_cols] = np.exp(data[numerical_cols])\n",
    "\n",
    "data['mean_numerical1'] = np.mean(data[numerical_cols],axis=1)\n",
    "data['std_numerical1'] = np.std(data[numerical_cols],axis=1)\n",
    "data['min_numerical1'] = np.min(data[numerical_cols],axis=1)\n",
    "data['max_numerical1'] = np.max(data[numerical_cols],axis=1)\n",
    "\n",
    "for cb in [('E2','E7'),('E9','E17'),('E5','E9')]:\n",
    "    data[cb[0]+'_plus_'+cb[1]] = data[cb[0]] + data[cb[1]]\n",
    "    \n",
    "for cb in [('E2','E7'),('E9','E17'),('E19','E9'),('E7','E9')]:\n",
    "    data[cb[0]+'_mul_'+cb[1]] = data[cb[0]] + data[cb[1]]\n",
    "    \n",
    "for cb in [('E17','E9'),('E9','E17'),('E5','E9'),('E7','E2'),('E9','E2')]:\n",
    "    data[cb[0]+'_devide_'+cb[1]] = data[cb[0]] + data[cb[1]]\n",
    "    \n",
    "count_feature = []\n",
    "for col in tqdm(A_ft+B_ft+C_ft+['E'+str(i) for i in [1,6,14,20,27]]):\n",
    "    data[col + \"_count\"] = data.groupby([col])[col].transform('count')\n",
    "    count_feature.append(col + \"_count\")\n",
    "    \n",
    "def vec(data,col1,col2):\n",
    "    dataword2vec2 = pd.concat((data[col1],data[col2]), axis=1)\n",
    "    dataword2vec3=np.array(dataword2vec2.astype(str))\n",
    "    dataword2vec3=dataword2vec3.tolist()  #必须用列表类型的数据才能训练词向量\n",
    "    model = Word2Vec(dataword2vec3, size=200,iter=15, hs=1, min_count=1, window=5,workers=6)\n",
    "    ws1=np.array(dataword2vec2[col1].astype('str'))\n",
    "    ws2=np.array(dataword2vec2[col2].astype('str'))\n",
    "    ws1=ws1.tolist()\n",
    "    ws2=ws2.tolist()\n",
    "    word2vecsim1=[]\n",
    "    \n",
    "    for i in tqdm(range(len(data))):\n",
    "        ws3=[ws1[i]]\n",
    "        ws4=[ws2[i]]\n",
    "        word2vecsim2=model.wv.n_similarity(ws3,ws4)#计算两列的相似度\n",
    "        word2vecsim1.append(word2vecsim2)\n",
    "    data[col1+col2+'_vec'] = np.array(word2vecsim1)\n",
    "    \n",
    "for cols in combinations(A_ft,2):\n",
    "    vec(data,cols[0],cols[1])\n",
    "for cols in combinations(B_ft,2):\n",
    "    vec(data,cols[0],cols[1])\n",
    "for cols in combinations(C_ft,2):\n",
    "    vec(data,cols[0],cols[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = [col for col in data.columns if col not in ['ID','label','date']+cat_list+['D1','D2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['mean_numerical2'] = np.mean(data[numerical_cols],axis=1)\n",
    "data['std_numerical2'] = np.std(data[numerical_cols],axis=1)\n",
    "data['min_numerical2'] = np.min(data[numerical_cols],axis=1)\n",
    "data['max_numerical2'] = np.max(data[numerical_cols],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useless_ft = ['E22','E3','E19']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name = list(set([col for col in data.columns if col not in useless_ft+['ID','label','date']]))\n",
    "#cat_list = A_ft+B_ft+['hour','day']+['E1','E14']\n",
    "print(feature_name)\n",
    "print(len(feature_name))\n",
    "print(cat_list)\n",
    "print(len(cat_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[cat_list] = data[cat_list].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time data = reduce_mem_usage(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_index = ~data['label'].isnull()\n",
    "X_train = data.loc[tr_index,:].reset_index(drop=True)\n",
    "y = data.loc[tr_index,:]['label'].reset_index(drop=True).astype(int)\n",
    "X_test = data[~tr_index].reset_index(drop=True)\n",
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cbt_cv(train_X,train_Y,test_X,params,feature_name=None,split=5,seed=20191031,cat_list=None,use_best=True,iterations=10000):\n",
    "    val_results = []\n",
    "    models_list = []\n",
    "    best_iterations = []\n",
    "    train_pred = np.zeros(train_X.shape[0])\n",
    "    test_pred = np.zeros(test_X.shape[0])\n",
    "    seeds=range(seed,seed+split)\n",
    "    \n",
    "    learning_rate = params['learning_rate']\n",
    "    depth = params['max_depth']\n",
    "    reg_lambda = params['reg_lambda']\n",
    "    bagging_temperature = params['bagging_temperature']\n",
    "    random_strength = params['random_strength']\n",
    "    \n",
    "    if feature_name == None:\n",
    "        feature_name = [col for col in train_X.columns if col not in ['ID','label','date']]\n",
    "    print('Using features:',feature_name)\n",
    "    print(len(feature_name)) \n",
    "    \n",
    "    train_val_spliter = StratifiedKFold(n_splits=split, random_state=seeds[0], shuffle=True)\n",
    "    \n",
    "    for index, (train_index, test_index) in enumerate(train_val_spliter.split(train_X, train_Y)):\n",
    "        print('fold:',index+1)\n",
    "        val_result = []\n",
    "\n",
    "        train_x, val_x, train_y, val_y = train_X[feature_name].iloc[train_index], train_X[feature_name].iloc[test_index], train_Y.iloc[train_index], train_Y.iloc[test_index]\n",
    "        \n",
    "        cbt_model = cbt.CatBoostClassifier(iterations=iterations,learning_rate=learning_rate,max_depth=depth,verbose=100,\n",
    "                                   early_stopping_rounds=700,task_type='GPU',eval_metric='AUC',loss_function='Logloss',\n",
    "                                   cat_features=cat_list,random_state=seeds[index],reg_lambda=reg_lambda,use_best_model=use_best)\n",
    "        cbt_model.fit(train_x[feature_name], train_y,eval_set=(val_x[feature_name],val_y))\n",
    "        gc.collect()\n",
    "        \n",
    "        train_pred[test_index] += cbt_model.predict_proba(val_x)[:,1]\n",
    "        fold_test_pred = cbt_model.predict_proba(X_test[feature_name])[:,1]\n",
    "        test_pred += fold_test_pred/split\n",
    "        \n",
    "        val_result.append(roc_auc_score(val_y, train_pred[test_index]))\n",
    "        print('AUC: ',val_result[-1])\n",
    "        val_result.append(log_loss(val_y, train_pred[test_index]))\n",
    "        print('log_loss: ',val_result[-1])\n",
    "        best_iterations.append(cbt_model.get_best_iteration())\n",
    "        val_results.append(val_result)\n",
    "        \n",
    "        del cbt_model\n",
    "        gc.collect()\n",
    "        \n",
    "    val_results = np.array(val_results)\n",
    "    print('cv completed')\n",
    "    print('mean best iteration: ',np.mean(best_iterations))\n",
    "    print('std best iteration: ',np.std(best_iterations))\n",
    "    print('oof AUC: ',roc_auc_score(train_Y,train_pred))\n",
    "    print('mean AUC: ',np.mean(val_results[:,0]))\n",
    "    print('std AUC: ',np.std(val_results[:,0]))\n",
    "    print('oof log_loss: ',log_loss(train_Y,train_pred))\n",
    "    print('mean log_loss: ',np.mean(val_results[:,1]))\n",
    "    print('std log_loss: ',np.std(val_results[:,1]))\n",
    "    return train_pred,test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_params = {'bagging_temperature': 1.0, \n",
    "          'learning_rate': 0.03, \n",
    "          'max_depth': 7, \n",
    "          'random_strength': 1.0,\n",
    "          'reg_lambda': 6.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df = pd.DataFrame()\n",
    "n_times = 12\n",
    "oof_df = pd.DataFrame()\n",
    "for i in [10*i+20191091 for i in range(n_times)]:\n",
    "    oof,pred = run_cbt_cv(X_train,y,X_test,params=default_params,feature_name=feature_name,seed=i,iterations=10000,use_best=True,split=5,cat_list=cat_list)\n",
    "    gc.collect()\n",
    "    \n",
    "    prediction_temp = pd.DataFrame()\n",
    "    prediction_temp['cbt_'+str(i)] = pred\n",
    "    prediction_df = pd.concat([prediction_df,prediction_temp],axis=1)\n",
    "    \n",
    "    oof_temp = pd.DataFrame()\n",
    "    oof_temp['cbt_'+str(i)] = oof\n",
    "    oof_df = pd.concat([oof_df,oof_temp],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_df.to_csv('oof_cbt_12.csv',index=False)\n",
    "prediction_df.to_csv('test_cbt_12.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
